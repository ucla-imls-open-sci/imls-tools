{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API Key and OPENAI Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads all of the packages that will be used and the OpenAI API Key that will be used. Store your OpenAI API Key in a .env file.\n",
    "If you are missing any of these packages run the following commands\n",
    "```\n",
    "$   pip3 install PyPDF2\n",
    "$   pip3 install python.dotenv\n",
    "$   pip3 install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"ORGANIZATION_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample API Call\n",
    "# response = openai.Completion.create(\n",
    "#     engine=\"text-davinci-002\",  # Specify the engine (model) you want to use\n",
    "#     prompt=\"Translate the following English text to French: 'Hello, how are you?'\",\n",
    "#     max_tokens=50,  # Limit the length of the generated text\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson Proposals will be extracted using the PyPDF2 Package. You must have the lessons downloaded locally for this code to work and place it inside of a \n",
    "Lesson_Proposal folder. It will extract all of the text and put it into the <i style=\"color: red\"><b>proposal_text</b></i> variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Lesson Proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the path of All the PDFs in the Lesson_Proposals Folder\n",
    "pdf_file_paths = []\n",
    "\n",
    "proposal_list = os.listdir(\"./Lesson_Proposals/\")\n",
    "\n",
    "# Extracts all the PDF Paths\n",
    "for i in proposal_list:\n",
    "    if i == \".DS_Store\":\n",
    "        continue\n",
    "    pdf_file_paths.append(f\"Lesson_Proposals/{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lesson_Proposals/114233851983_A_Path_to_Open_Inclusive_and_Collaborative_Science_for_Librarians.pdf', 'Lesson_Proposals/114233826722_Open_Hardware_for_librarians.pdf', 'Lesson_Proposals/114219657654_Data_Management_and_Sharing_Plans_for_Librarians_101.pdf', 'Lesson_Proposals/114229483598_Research Community Outreach with Open Science Team Agreements_Open_Science_Team_Agreements-Lessons_for_Librarians_in_Open_Science_Proposal.pdf', 'Lesson_Proposals/114232854610_Understanding_CARE_Principles_for_research_data.pdf', 'Lesson_Proposals/114233727582_Reproducible_research_workflows_2023_01_31.pdf', 'Lesson_Proposals/114205095243_Open_Qualitative_Research.pdf']\n"
     ]
    }
   ],
   "source": [
    "# print(pdf_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TEXT FROM EACH PROPOSAL\n",
    "\n",
    "lesson_proposal = []\n",
    "\n",
    "for i in pdf_file_paths:\n",
    "    pdf_file = open(i, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    text = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    pdf_file.close()\n",
    "    lesson_proposal.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Proposal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's API currently doesn't support a continuous chat as far as im concerned. There are a lot of repositories that address this problem but I think the solution I will be going with is a long continuous chain, starting with providing a rubric to score similiarities, providing an output format, then ultimately providing all of the proposals. We can test this with the online version GPT prior to using the API to see if it generates the result we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening a Chat Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Chat:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing a Rubric\n",
    "\n",
    "You can find the Rubric that was used to score similarity [here](https://docs.google.com/document/d/1x18mVubT2H4Gj_GvDM3nUupCqYQHaH8Qk8UleAQjPyU/edit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telling GPT to use this Rubric when Grading the Proposals\n",
    "\n",
    "rubric_message = \"\"\"\n",
    "The rubric will be displayed in the following format:\n",
    "Measurement:\n",
    "    Point Value. Description for that point value \n",
    "\n",
    "Keywords\n",
    "    1. Keywords are completely unrelated to each other and have no correlation.\n",
    "    2. Keywords can be seen in the same context in the english language\n",
    "    3. They contain one or two similar/same keywords\n",
    "    4. They contain very similar keywords but the context used throughout the proposal may be a little different \n",
    "    5. They have the same keywords with the addition/subtraction of one or more keywords\n",
    "Title\n",
    "    1. Titles have no correlation or do not cover similar topics\n",
    "    2. Titles cover the same branch of science\n",
    "    3. Titles contain similar words and cover the same topic\n",
    "Lesson Objectives\n",
    "    1. <30% of the lesson objectives align with each other\n",
    "    2. 30-50% of the lesson objectives align with each other\n",
    "    3.  50-70% of the lesson objectives align with each other\n",
    "    4. 70-90% of the lesson objectives align with each other\n",
    "    5. >90% of the lesson objectives align with each other\n",
    "Lesson Audience\n",
    "    1. Audience are completely different\n",
    "    2. Audience can be grouped into a similar room\n",
    "    3. Audience may be interested in the same subject\n",
    "    4. The lesson is heavily ingrained in both audienceâ€™s work/field\n",
    "    5. Teaching to the same audience\n",
    "Description\n",
    "    1. Descriptions are not similar with any context\n",
    "    2. Descriptions have similar words\n",
    "    3. Same concept is mentioned but different context surrounding the lesson creation\n",
    "    4. Same concept is mentioned as well as similar context for the creation of the lesson\n",
    "    5. Both lessons describe the same problem and issue that they plan to tackle\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rubric_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telling GPT How to Output the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_message = \"\"\"\n",
    "Please respond in the following format: \n",
    "\n",
    "1. Proposal 1 Title\n",
    "    Most Similar: (Proposal Most Similar to)\n",
    "        3-5 Sentences of Context comparing the 2 proposals\n",
    "        Rubric Score:\n",
    "            ...\n",
    "    Least Similar: (Proposal Least Similar to)\n",
    "        3-5 Sentences of Content comparing the 2 proposals\n",
    "        Rubric Score:\n",
    "            ...\n",
    "2. Proposal 2 Title\n",
    "    Same format as above\n",
    "3. The same format until all proposals have been considered\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all of the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take all of the different messages created above and combine them together into one singular prompt that we can send to GPT.\n",
    "It will HOPEFULLY output an appropriate response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"You are going to receive several lesson proposals and your job is to compare them to find proposals that are most similar to each other. \n",
    "You will have a total of 7 proposals, A, B, C, etc. You will compare the proposals and see which is the most similar and least similar to each individual \n",
    "proposal. Each proposal will go through an individual comparison against each other. You will use the rubric that will be provided as a way to score\n",
    "the similarities between each proposal. This score will be used in your response. Before analyzing and comparing the proposals, identify and separate each \n",
    "proposal into Title, Keypoints, Lesson Objectives, Lesson Audience, and Description. Then use the newly partitioned proposals in your analysis.\n",
    "\n",
    "The Rubric:\n",
    "{rubric_message}\n",
    "\n",
    "{format_message}\n",
    "\n",
    "Here are the following proposals\n",
    "\n",
    "Proposal 1:\n",
    "{lesson_proposal[0]}\n",
    "\n",
    "Proposal 2:\n",
    "{lesson_proposal[1]}\n",
    "\n",
    "Proposal 3:\n",
    "{lesson_proposal[2]}\n",
    "\n",
    "Proposal 4:\n",
    "{lesson_proposal[3]}\n",
    "\n",
    "Proposal 5:\n",
    "{lesson_proposal[4]}\n",
    "\n",
    "Proposal 6:\n",
    "{lesson_proposal[5]}\n",
    "\n",
    "Proposal 7:\n",
    "{lesson_proposal[6]}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OPENAI's API to prompt gpt-4 to analyze the text\n",
    "response = openai.ChatCompletion.create(model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the Message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will create a new file/overwrite the \"GPT_Analysis.txt\" with GPT's text analysis of the lesson proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response['choices'][0]['text'])\n",
    "GPT_Analysis = open(\"GPT_Analysis.txt\", 'w')\n",
    "GPT_Analysis.write(response.choices[0].message.content)\n",
    "GPT_Analysis.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
