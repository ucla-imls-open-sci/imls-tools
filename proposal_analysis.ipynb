{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load API Key and OPENAI Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads all of the packages that will be used and the OpenAI API Key that will be used. Store your OpenAI API Key in a .env file.\n",
    "If you are missing any of these packages run the following commands\n",
    "```\n",
    "$   pip3 install PyPDF2\n",
    "$   pip3 install python.dotenv\n",
    "$   pip3 install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import PyPDF2\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = os.getenv(\"ORGANIZATION_KEY\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample API Call\n",
    "# response = openai.Completion.create(\n",
    "#     engine=\"text-davinci-002\",  # Specify the engine (model) you want to use\n",
    "#     prompt=\"Translate the following English text to French: 'Hello, how are you?'\",\n",
    "#     max_tokens=50,  # Limit the length of the generated text\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Lesson Proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson Proposals will be extracted using the PyPDF2 Package. You must have the lessons downloaded locally for this code to work and place it inside of a \n",
    "Lesson_Proposal folder. It will extract all of the text and put it into the <i style=\"color: red\"><b>proposal_text</b></i> variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the path of All the PDFs in the Lesson_Proposals Folder\n",
    "pdf_file_paths = []\n",
    "\n",
    "proposal_list = os.listdir(\"./Lesson_Proposals/\")\n",
    "\n",
    "# Extracts all the PDF Paths\n",
    "for i in proposal_list:\n",
    "    pdf_file_paths.append(f\"Lesson_Proposals/{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT TEXT FROM EACH PROPOSAL\n",
    "\n",
    "proposal_text = []\n",
    "\n",
    "for i in pdf_file_paths:\n",
    "    pdf_file = open(i, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    text = ''\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text += page.extract_text()\n",
    "    \n",
    "    pdf_file.close()\n",
    "    proposal_text.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting GPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's API currently doesn't support a continuous chat as far as im concerned. There are a lot of repositories that address this problem but I think the solution I will be going with is a long continuous chain, starting with providing a rubric to score similiarities, providing an output format, then ultimately providing all of the proposals. We can test this with the online version GPT prior to using the API to see if it generates the result we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening a Chat Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing a Rubric\n",
    "\n",
    "You can find the Rubric that was used to score similarity [here](https://docs.google.com/document/d/1x18mVubT2H4Gj_GvDM3nUupCqYQHaH8Qk8UleAQjPyU/edit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telling GPT to use this Rubric when Grading the Proposals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telling GPT How to Output the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_message = \"\"\"\n",
    "Please format your response in the following format: \n",
    "\n",
    "1. Proposal Title 1\n",
    "    Most Similar: (Proposal Most Similar to)\n",
    "        3-5 Sentences of Context comparing the 2 proposals\n",
    "        Rubric Score:\n",
    "            ...\n",
    "    Least Similar: (Proposal Least Similar to)\n",
    "        3-5 Sentences of Content comparing the 2 proposals\n",
    "        Rubric Score:\n",
    "            ...\n",
    "2. Proposal Title 2\n",
    "    Same format as above\n",
    "3. The same format until all proposals have been considered\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
